{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sUtoed20cRJJ"
   },
   "source": [
    "# 使用 tf.data 加载文本数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NWeQAo0Ec_BL"
   },
   "source": [
    "本教程为你提供了一个如何使用 `tf.data.TextLineDataset` 来加载文本文件的示例。`TextLineDataset` 通常被用来以文本文件构建数据集（原文件中的一行为一个样本) 。这适用于大多数的基于行的文本数据（例如，诗歌或错误日志) 。下面我们将使用相同作品（荷马的伊利亚特）三个不同版本的英文翻译，然后训练一个模型来通过单行文本确定译者。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgZ9gjmPfSnK"
   },
   "source": [
    "## 环境搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "baYFZMW_bJHh"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YWVWjyIkffau"
   },
   "source": [
    "三个版本的翻译分别来自于:\n",
    "\n",
    " - [William Cowper](https://en.wikipedia.org/wiki/William_Cowper) — [text](https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt)\n",
    "\n",
    " - [Edward, Earl of Derby](https://en.wikipedia.org/wiki/Edward_Smith-Stanley,_14th_Earl_of_Derby) — [text](https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt)\n",
    "\n",
    "- [Samuel Butler](https://en.wikipedia.org/wiki/Samuel_Butler_%28novelist%29) — [text](https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt)\n",
    "\n",
    "本教程中使用的文本文件已经进行过一些典型的预处理，主要包括删除了文档页眉和页脚，行号，章节标题。请下载这些已经被局部改动过的文件。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YlKQthEYlFw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\wangxingda\\\\.keras\\\\datasets'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
    "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
    "\n",
    "for name in FILE_NAMES:\n",
    "  text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)\n",
    "  \n",
    "parent_dir = os.path.dirname(text_dir)\n",
    "\n",
    "parent_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q3sDy6nuXoNp"
   },
   "source": [
    "## 将文本加载到数据集中\n",
    "\n",
    "迭代整个文件，将整个文件加载到自己的数据集中。\n",
    "\n",
    "每个样本都需要单独标记，所以请使用 `tf.data.Dataset.map` 来为每个样本设定标签。这将迭代数据集中的每一个样本并且返回（ `example, label` ）对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K0BjCOpOh7Ch"
   },
   "outputs": [],
   "source": [
    "def labeler(example, index):\n",
    "  return example, tf.cast(index, tf.int64)  \n",
    "\n",
    "labeled_data_sets = []\n",
    "\n",
    "for i, file_name in enumerate(FILE_NAMES):\n",
    "  lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))\n",
    "  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
    "  labeled_data_sets.append(labeled_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M8PHK5J_cXE5"
   },
   "source": [
    "将这些标记的数据集合并到一个数据集中，然后对其进行随机化操作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6jAeYkTIi9-2"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TAKE_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qd544E-Sh63L"
   },
   "outputs": [],
   "source": [
    "all_labeled_data = labeled_data_sets[0]\n",
    "for labeled_dataset in labeled_data_sets[1:]:\n",
    "  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
    "  \n",
    "all_labeled_data = all_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r4JEHrJXeG5k"
   },
   "source": [
    "你可以使用 `tf.data.Dataset.take` 与 `print` 来查看 `(example, label)` 对的外观。`numpy` 属性显示每个 Tensor 的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gywKlN0xh6u5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=620469, shape=(), dtype=string, numpy=b\"And, stripping slain Patroclus, thought'st thee safe,\">, <tf.Tensor: id=620470, shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: id=620471, shape=(), dtype=string, numpy=b'stock of all kinds would come from far and near to water; here, then,'>, <tf.Tensor: id=620472, shape=(), dtype=int64, numpy=2>)\n",
      "(<tf.Tensor: id=620473, shape=(), dtype=string, numpy=b\"take Amphimachus's helmet from off his temples, and in a moment Ajax\">, <tf.Tensor: id=620474, shape=(), dtype=int64, numpy=2>)\n",
      "(<tf.Tensor: id=620475, shape=(), dtype=string, numpy=b'But when they came, at length, where Xanthus winds'>, <tf.Tensor: id=620476, shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: id=620477, shape=(), dtype=string, numpy=b'better, that earth should open and swallow us here in this place, than'>, <tf.Tensor: id=620478, shape=(), dtype=int64, numpy=2>)\n"
     ]
    }
   ],
   "source": [
    "for ex in all_labeled_data.take(5):\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rrpU2_sfDh0"
   },
   "source": [
    "## 将文本编码成数字\n",
    "\n",
    "机器学习基于的是数字而非文本，所以字符串需要被转化成数字列表。\n",
    "为了达到此目的，我们需要构建文本与整数的一一映射。\n",
    "\n",
    "### 建立词汇表\n",
    "\n",
    "\n",
    "首先，通过将文本标记为单独的单词集合来构建词汇表。在 TensorFlow 和 Python 中均有很多方法来达成这一目的。在本教程中:\n",
    "\n",
    "1. 迭代每个样本的 `numpy` 值。\n",
    "2. 使用 `tfds.features.text.Tokenizer` 来将其分割成 `token`。\n",
    "3. 将这些 `token` 放入一个 Python 集合中，借此来清除重复项。\n",
    "4. 获取该词汇表的大小以便于以后使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YkHtbGnDh6mg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17178"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "for text_tensor, _ in all_labeled_data:\n",
    "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "  vocabulary_set.update(some_tokens)\n",
    "\n",
    "vocab_size = len(vocabulary_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0W35VJqAh9zs"
   },
   "source": [
    "### 样本编码\n",
    "\n",
    "通过传递 `vocabulary_set` 到 `tfds.features.text.TokenTextEncoder` 来构建一个编码器。编码器的 `encode` 方法传入一行文本，返回一个整数列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gkxJIVAth6j0"
   },
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6S5Qyabi-vo"
   },
   "source": [
    "你可以尝试运行这一行代码并查看输出的样式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jgxPZaxUuTbk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"And, stripping slain Patroclus, thought'st thee safe,\"\n"
     ]
    }
   ],
   "source": [
    "example_text = next(iter(all_labeled_data))[0].numpy()\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XoVpKR3qj5yb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3571, 1104, 985, 16725, 8867, 9302, 757, 8239]\n"
     ]
    }
   ],
   "source": [
    "encoded_example = encoder.encode(example_text)\n",
    "print(encoded_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p9qHM0v8k_Mg"
   },
   "source": [
    "现在，在数据集上运行编码器（通过将编码器打包到 `tf.py_function` 并且传参至数据集的 `map` 方法的方式来运行）。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HcIQ7LOTh6eT"
   },
   "outputs": [],
   "source": [
    "def encode(text_tensor, label):\n",
    "  encoded_text = encoder.encode(text_tensor.numpy())\n",
    "  return encoded_text, label\n",
    "\n",
    "def encode_map_fn(text, label):\n",
    "  return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))\n",
    "\n",
    "all_encoded_data = all_labeled_data.map(encode_map_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_YZToSXSm0qr"
   },
   "source": [
    "## 将数据集分割为测试集和训练集且进行分支\n",
    "\n",
    "使用 `tf.data.Dataset.take` 和 `tf.data.Dataset.skip` 来建立一个小一些的测试数据集和稍大一些的训练数据集。\n",
    "\n",
    "在数据集被传入模型之前，数据集需要被分批。最典型的是，每个分支中的样本大小与格式需要一致。但是数据集中样本并不全是相同大小的（每行文本字数并不相同）。因此，使用 `tf.data.Dataset.padded_batch`（而不是 `batch` ）将样本填充到相同的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-rmbijQh6bf"
   },
   "outputs": [],
   "source": [
    "train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\n",
    "train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))\n",
    "\n",
    "test_data = all_encoded_data.take(TAKE_SIZE)\n",
    "test_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xdz7SVwmqi1l"
   },
   "source": [
    "\n",
    "现在，test_data 和 train_data 不是（ `example, label` ）对的集合，而是批次的集合。每个批次都是一对（*多样本*, *多标签* ），表示为数组。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kMslWfuwoqpB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=719942, shape=(16,), dtype=int64, numpy=\n",
       " array([ 3571,  1104,   985, 16725,  8867,  9302,   757,  8239,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0], dtype=int64)>,\n",
       " <tf.Tensor: id=719946, shape=(), dtype=int64, numpy=0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text, sample_labels = next(iter(test_data))\n",
    "\n",
    "sample_text[0], sample_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UI4I6_Sa0vWu"
   },
   "source": [
    "\n",
    "由于我们引入了一个新的 token 来编码（填充零），因此词汇表大小增加了一个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlD1Lli91vuc"
   },
   "outputs": [],
   "source": [
    "vocab_size += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8SUhGFNsmRi"
   },
   "source": [
    "## 建立模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QJgI1pow2YR9"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wi0iiKLTKdoF"
   },
   "source": [
    "第一层将整数表示转换为密集矢量嵌入。更多内容请查阅 [Word Embeddings](../../tutorials/sequences/word_embeddings) 教程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DR6-ctbY638P"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Embedding(vocab_size, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_8OJOPohKh1q"
   },
   "source": [
    "\n",
    "下一层是 [LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) 层，它允许模型利用上下文中理解单词含义。 LSTM 上的双向包装器有助于模型理解当前数据点与其之前和之后的数据点的关系。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6rnq6DN_WUs"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cdffbMr5LF1g"
   },
   "source": [
    "\n",
    "最后，我们将获得一个或多个紧密连接的层，其中最后一层是输出层。输出层输出样本属于各个标签的概率，最后具有最高概率的分类标签即为最终预测结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QTEaNSnLCsv5"
   },
   "outputs": [],
   "source": [
    "# 一个或多个紧密连接的层\n",
    "# 编辑 `for` 行的列表去检测层的大小\n",
    "for units in [64, 64]:\n",
    "  model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
    "\n",
    "# 输出层。第一个参数是标签个数。\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zLHPU8q5DLi_"
   },
   "source": [
    "最后，编译这个模型。对于一个 softmax 分类模型来说，通常使用 `sparse_categorical_crossentropy` 作为其损失函数。你可以尝试其他的优化器，但是 `adam` 是最常用的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pkTBUVO4h6Y5"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DM-HLo5NDhql"
   },
   "source": [
    "## 训练模型\n",
    "\n",
    "利用提供的数据训练出的模型有着不错的精度（大约 83% ）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aLtO33tNh6V8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    168/Unknown - 18s 18s/step - loss: 1.0979 - accuracy: 0.43 - 18s 9s/step - loss: 1.0977 - accuracy: 0.3828 - 18s 6s/step - loss: 1.0961 - accuracy: 0.369 - 18s 4s/step - loss: 1.0936 - accuracy: 0.367 - 18s 4s/step - loss: 1.0920 - accuracy: 0.387 - 18s 3s/step - loss: 1.0919 - accuracy: 0.375 - 18s 3s/step - loss: 1.0892 - accuracy: 0.386 - 18s 2s/step - loss: 1.0868 - accuracy: 0.388 - 18s 2s/step - loss: 1.0853 - accuracy: 0.378 - 18s 2s/step - loss: 1.0821 - accuracy: 0.390 - 18s 2s/step - loss: 1.0791 - accuracy: 0.390 - 18s 1s/step - loss: 1.0762 - accuracy: 0.394 - 18s 1s/step - loss: 1.0720 - accuracy: 0.400 - 18s 1s/step - loss: 1.0699 - accuracy: 0.395 - 18s 1s/step - loss: 1.0644 - accuracy: 0.403 - 18s 1s/step - loss: 1.0595 - accuracy: 0.399 - 18s 1s/step - loss: 1.0547 - accuracy: 0.402 - 18s 1s/step - loss: 1.0475 - accuracy: 0.410 - 18s 956ms/step - loss: 1.0413 - accuracy: 0.41 - 18s 910ms/step - loss: 1.0353 - accuracy: 0.42 - 18s 868ms/step - loss: 1.0294 - accuracy: 0.43 - 18s 830ms/step - loss: 1.0229 - accuracy: 0.43 - 18s 795ms/step - loss: 1.0145 - accuracy: 0.44 - 18s 764ms/step - loss: 1.0071 - accuracy: 0.44 - 18s 734ms/step - loss: 1.0005 - accuracy: 0.45 - 18s 707ms/step - loss: 0.9900 - accuracy: 0.46 - 18s 682ms/step - loss: 0.9813 - accuracy: 0.47 - 18s 659ms/step - loss: 0.9706 - accuracy: 0.47 - 18s 638ms/step - loss: 0.9741 - accuracy: 0.48 - 19s 617ms/step - loss: 0.9642 - accuracy: 0.48 - 19s 598ms/step - loss: 0.9530 - accuracy: 0.49 - 19s 581ms/step - loss: 0.9425 - accuracy: 0.49 - 19s 564ms/step - loss: 0.9316 - accuracy: 0.50 - 19s 549ms/step - loss: 0.9259 - accuracy: 0.50 - 19s 534ms/step - loss: 0.9173 - accuracy: 0.51 - 19s 520ms/step - loss: 0.9118 - accuracy: 0.51 - 19s 507ms/step - loss: 0.9100 - accuracy: 0.51 - 19s 495ms/step - loss: 0.9041 - accuracy: 0.51 - 19s 483ms/step - loss: 0.8958 - accuracy: 0.51 - 19s 472ms/step - loss: 0.8923 - accuracy: 0.52 - 19s 461ms/step - loss: 0.8871 - accuracy: 0.52 - 19s 451ms/step - loss: 0.8824 - accuracy: 0.52 - 19s 441ms/step - loss: 0.8780 - accuracy: 0.52 - 19s 432ms/step - loss: 0.8750 - accuracy: 0.52 - 19s 423ms/step - loss: 0.8683 - accuracy: 0.53 - 19s 415ms/step - loss: 0.8637 - accuracy: 0.53 - 19s 406ms/step - loss: 0.8599 - accuracy: 0.53 - 19s 399ms/step - loss: 0.8559 - accuracy: 0.52 - 19s 391ms/step - loss: 0.8526 - accuracy: 0.53 - 19s 384ms/step - loss: 0.8534 - accuracy: 0.53 - 19s 377ms/step - loss: 0.8506 - accuracy: 0.53 - 19s 370ms/step - loss: 0.8469 - accuracy: 0.53 - 19s 364ms/step - loss: 0.8443 - accuracy: 0.53 - 19s 358ms/step - loss: 0.8437 - accuracy: 0.53 - 19s 352ms/step - loss: 0.8432 - accuracy: 0.53 - 19s 347ms/step - loss: 0.8399 - accuracy: 0.53 - 19s 341ms/step - loss: 0.8350 - accuracy: 0.54 - 19s 336ms/step - loss: 0.8330 - accuracy: 0.54 - 20s 331ms/step - loss: 0.8294 - accuracy: 0.54 - 20s 326ms/step - loss: 0.8268 - accuracy: 0.54 - 20s 321ms/step - loss: 0.8244 - accuracy: 0.54 - 20s 316ms/step - loss: 0.8214 - accuracy: 0.54 - 20s 312ms/step - loss: 0.8198 - accuracy: 0.54 - 20s 307ms/step - loss: 0.8179 - accuracy: 0.54 - 20s 303ms/step - loss: 0.8157 - accuracy: 0.54 - 20s 299ms/step - loss: 0.8133 - accuracy: 0.54 - 20s 295ms/step - loss: 0.8118 - accuracy: 0.54 - 20s 291ms/step - loss: 0.8099 - accuracy: 0.54 - 20s 288ms/step - loss: 0.8072 - accuracy: 0.55 - 20s 284ms/step - loss: 0.8042 - accuracy: 0.55 - 20s 281ms/step - loss: 0.8030 - accuracy: 0.55 - 20s 277ms/step - loss: 0.8025 - accuracy: 0.55 - 20s 274ms/step - loss: 0.8013 - accuracy: 0.55 - 20s 271ms/step - loss: 0.7990 - accuracy: 0.55 - 20s 268ms/step - loss: 0.7969 - accuracy: 0.55 - 20s 265ms/step - loss: 0.7935 - accuracy: 0.55 - 20s 262ms/step - loss: 0.7935 - accuracy: 0.55 - 20s 259ms/step - loss: 0.7913 - accuracy: 0.56 - 20s 256ms/step - loss: 0.7888 - accuracy: 0.56 - 20s 253ms/step - loss: 0.7879 - accuracy: 0.56 - 20s 250ms/step - loss: 0.7853 - accuracy: 0.56 - 20s 248ms/step - loss: 0.7835 - accuracy: 0.56 - 20s 245ms/step - loss: 0.7824 - accuracy: 0.56 - 20s 243ms/step - loss: 0.7819 - accuracy: 0.56 - 20s 240ms/step - loss: 0.7791 - accuracy: 0.56 - 20s 238ms/step - loss: 0.7763 - accuracy: 0.56 - 20s 235ms/step - loss: 0.7739 - accuracy: 0.56 - 21s 233ms/step - loss: 0.7727 - accuracy: 0.56 - 21s 231ms/step - loss: 0.7708 - accuracy: 0.56 - 21s 229ms/step - loss: 0.7693 - accuracy: 0.56 - 21s 227ms/step - loss: 0.7674 - accuracy: 0.56 - 21s 225ms/step - loss: 0.7652 - accuracy: 0.57 - 21s 223ms/step - loss: 0.7637 - accuracy: 0.57 - 21s 221ms/step - loss: 0.7620 - accuracy: 0.57 - 21s 219ms/step - loss: 0.7604 - accuracy: 0.57 - 21s 217ms/step - loss: 0.7579 - accuracy: 0.57 - 21s 215ms/step - loss: 0.7560 - accuracy: 0.57 - 21s 213ms/step - loss: 0.7541 - accuracy: 0.57 - 21s 212ms/step - loss: 0.7523 - accuracy: 0.57 - 21s 210ms/step - loss: 0.7529 - accuracy: 0.57 - 21s 208ms/step - loss: 0.7528 - accuracy: 0.57 - 21s 207ms/step - loss: 0.7511 - accuracy: 0.57 - 21s 205ms/step - loss: 0.7508 - accuracy: 0.57 - 21s 203ms/step - loss: 0.7511 - accuracy: 0.57 - 21s 202ms/step - loss: 0.7502 - accuracy: 0.58 - 21s 200ms/step - loss: 0.7497 - accuracy: 0.58 - 21s 199ms/step - loss: 0.7487 - accuracy: 0.58 - 21s 197ms/step - loss: 0.7474 - accuracy: 0.58 - 21s 196ms/step - loss: 0.7461 - accuracy: 0.58 - 21s 194ms/step - loss: 0.7445 - accuracy: 0.58 - 21s 193ms/step - loss: 0.7432 - accuracy: 0.58 - 21s 192ms/step - loss: 0.7414 - accuracy: 0.58 - 22s 190ms/step - loss: 0.7404 - accuracy: 0.58 - 22s 189ms/step - loss: 0.7385 - accuracy: 0.58 - 22s 188ms/step - loss: 0.7377 - accuracy: 0.58 - 22s 186ms/step - loss: 0.7367 - accuracy: 0.58 - 22s 185ms/step - loss: 0.7353 - accuracy: 0.59 - 22s 184ms/step - loss: 0.7349 - accuracy: 0.59 - 22s 183ms/step - loss: 0.7333 - accuracy: 0.59 - 22s 182ms/step - loss: 0.7329 - accuracy: 0.59 - 22s 180ms/step - loss: 0.7330 - accuracy: 0.59 - 22s 179ms/step - loss: 0.7325 - accuracy: 0.59 - 22s 178ms/step - loss: 0.7311 - accuracy: 0.59 - 22s 177ms/step - loss: 0.7311 - accuracy: 0.59 - 22s 176ms/step - loss: 0.7298 - accuracy: 0.59 - 22s 175ms/step - loss: 0.7288 - accuracy: 0.59 - 22s 174ms/step - loss: 0.7279 - accuracy: 0.59 - 22s 173ms/step - loss: 0.7270 - accuracy: 0.59 - 22s 172ms/step - loss: 0.7269 - accuracy: 0.59 - 22s 171ms/step - loss: 0.7250 - accuracy: 0.59 - 22s 170ms/step - loss: 0.7246 - accuracy: 0.59 - 22s 169ms/step - loss: 0.7232 - accuracy: 0.59 - 22s 168ms/step - loss: 0.7224 - accuracy: 0.60 - 22s 167ms/step - loss: 0.7226 - accuracy: 0.60 - 22s 166ms/step - loss: 0.7213 - accuracy: 0.60 - 22s 165ms/step - loss: 0.7204 - accuracy: 0.60 - 22s 164ms/step - loss: 0.7192 - accuracy: 0.60 - 22s 163ms/step - loss: 0.7187 - accuracy: 0.60 - 23s 162ms/step - loss: 0.7176 - accuracy: 0.60 - 23s 161ms/step - loss: 0.7172 - accuracy: 0.60 - 23s 160ms/step - loss: 0.7161 - accuracy: 0.60 - 23s 159ms/step - loss: 0.7159 - accuracy: 0.60 - 23s 159ms/step - loss: 0.7158 - accuracy: 0.60 - 23s 158ms/step - loss: 0.7159 - accuracy: 0.60 - 23s 157ms/step - loss: 0.7158 - accuracy: 0.60 - 23s 156ms/step - loss: 0.7154 - accuracy: 0.60 - 23s 155ms/step - loss: 0.7144 - accuracy: 0.60 - 23s 154ms/step - loss: 0.7138 - accuracy: 0.60 - 23s 154ms/step - loss: 0.7129 - accuracy: 0.60 - 23s 153ms/step - loss: 0.7120 - accuracy: 0.60 - 23s 152ms/step - loss: 0.7119 - accuracy: 0.61 - 23s 151ms/step - loss: 0.7117 - accuracy: 0.61 - 23s 150ms/step - loss: 0.7107 - accuracy: 0.61 - 23s 150ms/step - loss: 0.7098 - accuracy: 0.61 - 23s 149ms/step - loss: 0.7095 - accuracy: 0.61 - 23s 148ms/step - loss: 0.7082 - accuracy: 0.61 - 23s 148ms/step - loss: 0.7086 - accuracy: 0.61 - 23s 147ms/step - loss: 0.7072 - accuracy: 0.61 - 23s 146ms/step - loss: 0.7066 - accuracy: 0.61 - 23s 145ms/step - loss: 0.7063 - accuracy: 0.61 - 23s 145ms/step - loss: 0.7065 - accuracy: 0.61 - 23s 144ms/step - loss: 0.7061 - accuracy: 0.61 - 23s 143ms/step - loss: 0.7054 - accuracy: 0.61 - 23s 143ms/step - loss: 0.7051 - accuracy: 0.61 - 23s 142ms/step - loss: 0.7042 - accuracy: 0.61 - 23s 141ms/step - loss: 0.7039 - accuracy: 0.61 - 24s 141ms/step - loss: 0.7036 - accuracy: 0.61 - 24s 140ms/step - loss: 0.7024 - accuracy: 0.6183\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    335/Unknown - 24s 140ms/step - loss: 0.7026 - accuracy: 0.61 - 24s 139ms/step - loss: 0.7019 - accuracy: 0.61 - 24s 138ms/step - loss: 0.7012 - accuracy: 0.61 - 24s 138ms/step - loss: 0.7007 - accuracy: 0.61 - 24s 137ms/step - loss: 0.7005 - accuracy: 0.61 - 24s 137ms/step - loss: 0.6997 - accuracy: 0.62 - 24s 136ms/step - loss: 0.6985 - accuracy: 0.62 - 24s 136ms/step - loss: 0.6975 - accuracy: 0.62 - 24s 135ms/step - loss: 0.6976 - accuracy: 0.62 - 24s 134ms/step - loss: 0.6978 - accuracy: 0.62 - 24s 134ms/step - loss: 0.6967 - accuracy: 0.62 - 24s 133ms/step - loss: 0.6960 - accuracy: 0.62 - 24s 133ms/step - loss: 0.6947 - accuracy: 0.62 - 24s 132ms/step - loss: 0.6939 - accuracy: 0.62 - 24s 132ms/step - loss: 0.6934 - accuracy: 0.62 - 24s 131ms/step - loss: 0.6932 - accuracy: 0.62 - 24s 131ms/step - loss: 0.6925 - accuracy: 0.62 - 24s 130ms/step - loss: 0.6918 - accuracy: 0.62 - 24s 130ms/step - loss: 0.6910 - accuracy: 0.62 - 24s 129ms/step - loss: 0.6902 - accuracy: 0.62 - 24s 129ms/step - loss: 0.6900 - accuracy: 0.62 - 24s 128ms/step - loss: 0.6892 - accuracy: 0.62 - 24s 128ms/step - loss: 0.6884 - accuracy: 0.62 - 24s 127ms/step - loss: 0.6884 - accuracy: 0.62 - 24s 127ms/step - loss: 0.6873 - accuracy: 0.62 - 25s 126ms/step - loss: 0.6867 - accuracy: 0.62 - 25s 126ms/step - loss: 0.6860 - accuracy: 0.63 - 25s 125ms/step - loss: 0.6855 - accuracy: 0.63 - 25s 125ms/step - loss: 0.6853 - accuracy: 0.63 - 25s 125ms/step - loss: 0.6842 - accuracy: 0.63 - 25s 124ms/step - loss: 0.6834 - accuracy: 0.63 - 25s 124ms/step - loss: 0.6825 - accuracy: 0.63 - 25s 123ms/step - loss: 0.6823 - accuracy: 0.63 - 25s 123ms/step - loss: 0.6817 - accuracy: 0.63 - 25s 122ms/step - loss: 0.6816 - accuracy: 0.63 - 25s 122ms/step - loss: 0.6809 - accuracy: 0.63 - 25s 121ms/step - loss: 0.6811 - accuracy: 0.63 - 25s 121ms/step - loss: 0.6816 - accuracy: 0.63 - 25s 121ms/step - loss: 0.6807 - accuracy: 0.63 - 25s 120ms/step - loss: 0.6808 - accuracy: 0.63 - 25s 120ms/step - loss: 0.6809 - accuracy: 0.63 - 25s 119ms/step - loss: 0.6804 - accuracy: 0.63 - 25s 119ms/step - loss: 0.6805 - accuracy: 0.63 - 25s 119ms/step - loss: 0.6804 - accuracy: 0.63 - 25s 118ms/step - loss: 0.6804 - accuracy: 0.63 - 25s 118ms/step - loss: 0.6810 - accuracy: 0.63 - 25s 118ms/step - loss: 0.6805 - accuracy: 0.63 - 25s 117ms/step - loss: 0.6803 - accuracy: 0.63 - 25s 117ms/step - loss: 0.6797 - accuracy: 0.63 - 25s 116ms/step - loss: 0.6791 - accuracy: 0.63 - 25s 116ms/step - loss: 0.6785 - accuracy: 0.63 - 25s 116ms/step - loss: 0.6782 - accuracy: 0.63 - 25s 115ms/step - loss: 0.6774 - accuracy: 0.63 - 26s 115ms/step - loss: 0.6767 - accuracy: 0.63 - 26s 115ms/step - loss: 0.6762 - accuracy: 0.63 - 26s 114ms/step - loss: 0.6758 - accuracy: 0.63 - 26s 114ms/step - loss: 0.6756 - accuracy: 0.63 - 26s 114ms/step - loss: 0.6755 - accuracy: 0.63 - 26s 113ms/step - loss: 0.6752 - accuracy: 0.63 - 26s 113ms/step - loss: 0.6748 - accuracy: 0.63 - 26s 113ms/step - loss: 0.6744 - accuracy: 0.63 - 26s 112ms/step - loss: 0.6735 - accuracy: 0.64 - 26s 112ms/step - loss: 0.6734 - accuracy: 0.64 - 26s 112ms/step - loss: 0.6726 - accuracy: 0.64 - 26s 111ms/step - loss: 0.6730 - accuracy: 0.64 - 26s 111ms/step - loss: 0.6730 - accuracy: 0.64 - 26s 111ms/step - loss: 0.6722 - accuracy: 0.64 - 26s 110ms/step - loss: 0.6720 - accuracy: 0.64 - 26s 110ms/step - loss: 0.6715 - accuracy: 0.64 - 26s 110ms/step - loss: 0.6709 - accuracy: 0.64 - 26s 109ms/step - loss: 0.6702 - accuracy: 0.64 - 26s 109ms/step - loss: 0.6692 - accuracy: 0.64 - 26s 109ms/step - loss: 0.6686 - accuracy: 0.64 - 26s 108ms/step - loss: 0.6688 - accuracy: 0.64 - 26s 108ms/step - loss: 0.6687 - accuracy: 0.64 - 26s 108ms/step - loss: 0.6679 - accuracy: 0.64 - 26s 108ms/step - loss: 0.6670 - accuracy: 0.64 - 26s 107ms/step - loss: 0.6665 - accuracy: 0.64 - 26s 107ms/step - loss: 0.6665 - accuracy: 0.64 - 26s 107ms/step - loss: 0.6659 - accuracy: 0.64 - 26s 106ms/step - loss: 0.6651 - accuracy: 0.64 - 27s 106ms/step - loss: 0.6642 - accuracy: 0.64 - 27s 106ms/step - loss: 0.6632 - accuracy: 0.64 - 27s 106ms/step - loss: 0.6627 - accuracy: 0.64 - 27s 105ms/step - loss: 0.6623 - accuracy: 0.64 - 27s 105ms/step - loss: 0.6621 - accuracy: 0.64 - 27s 105ms/step - loss: 0.6618 - accuracy: 0.64 - 27s 104ms/step - loss: 0.6617 - accuracy: 0.64 - 27s 104ms/step - loss: 0.6610 - accuracy: 0.64 - 27s 104ms/step - loss: 0.6604 - accuracy: 0.65 - 27s 104ms/step - loss: 0.6601 - accuracy: 0.65 - 27s 103ms/step - loss: 0.6591 - accuracy: 0.65 - 27s 103ms/step - loss: 0.6583 - accuracy: 0.65 - 27s 103ms/step - loss: 0.6575 - accuracy: 0.65 - 27s 103ms/step - loss: 0.6573 - accuracy: 0.65 - 27s 102ms/step - loss: 0.6568 - accuracy: 0.65 - 27s 102ms/step - loss: 0.6559 - accuracy: 0.65 - 27s 102ms/step - loss: 0.6550 - accuracy: 0.65 - 27s 102ms/step - loss: 0.6543 - accuracy: 0.65 - 27s 101ms/step - loss: 0.6537 - accuracy: 0.65 - 27s 101ms/step - loss: 0.6535 - accuracy: 0.65 - 27s 101ms/step - loss: 0.6533 - accuracy: 0.65 - 27s 101ms/step - loss: 0.6532 - accuracy: 0.65 - 27s 100ms/step - loss: 0.6520 - accuracy: 0.65 - 27s 100ms/step - loss: 0.6515 - accuracy: 0.65 - 27s 100ms/step - loss: 0.6505 - accuracy: 0.65 - 27s 100ms/step - loss: 0.6500 - accuracy: 0.65 - 27s 100ms/step - loss: 0.6487 - accuracy: 0.65 - 28s 99ms/step - loss: 0.6482 - accuracy: 0.6584 - 28s 99ms/step - loss: 0.6475 - accuracy: 0.658 - 28s 99ms/step - loss: 0.6467 - accuracy: 0.659 - 28s 99ms/step - loss: 0.6460 - accuracy: 0.659 - 28s 98ms/step - loss: 0.6461 - accuracy: 0.659 - 28s 98ms/step - loss: 0.6454 - accuracy: 0.660 - 28s 98ms/step - loss: 0.6452 - accuracy: 0.660 - 28s 98ms/step - loss: 0.6447 - accuracy: 0.660 - 28s 98ms/step - loss: 0.6442 - accuracy: 0.661 - 28s 97ms/step - loss: 0.6438 - accuracy: 0.661 - 28s 97ms/step - loss: 0.6440 - accuracy: 0.661 - 28s 97ms/step - loss: 0.6433 - accuracy: 0.662 - 28s 97ms/step - loss: 0.6427 - accuracy: 0.662 - 28s 97ms/step - loss: 0.6422 - accuracy: 0.663 - 28s 96ms/step - loss: 0.6422 - accuracy: 0.663 - 28s 96ms/step - loss: 0.6416 - accuracy: 0.663 - 28s 96ms/step - loss: 0.6413 - accuracy: 0.663 - 28s 96ms/step - loss: 0.6405 - accuracy: 0.664 - 28s 95ms/step - loss: 0.6397 - accuracy: 0.664 - 28s 95ms/step - loss: 0.6392 - accuracy: 0.664 - 28s 95ms/step - loss: 0.6385 - accuracy: 0.665 - 28s 95ms/step - loss: 0.6381 - accuracy: 0.665 - 28s 95ms/step - loss: 0.6377 - accuracy: 0.666 - 28s 94ms/step - loss: 0.6377 - accuracy: 0.666 - 28s 94ms/step - loss: 0.6372 - accuracy: 0.666 - 28s 94ms/step - loss: 0.6368 - accuracy: 0.667 - 28s 94ms/step - loss: 0.6361 - accuracy: 0.667 - 28s 94ms/step - loss: 0.6359 - accuracy: 0.667 - 29s 94ms/step - loss: 0.6356 - accuracy: 0.667 - 29s 93ms/step - loss: 0.6350 - accuracy: 0.668 - 29s 93ms/step - loss: 0.6341 - accuracy: 0.669 - 29s 93ms/step - loss: 0.6341 - accuracy: 0.669 - 29s 93ms/step - loss: 0.6333 - accuracy: 0.669 - 29s 93ms/step - loss: 0.6326 - accuracy: 0.670 - 29s 92ms/step - loss: 0.6324 - accuracy: 0.670 - 29s 92ms/step - loss: 0.6321 - accuracy: 0.670 - 29s 92ms/step - loss: 0.6319 - accuracy: 0.671 - 29s 92ms/step - loss: 0.6316 - accuracy: 0.671 - 29s 92ms/step - loss: 0.6312 - accuracy: 0.671 - 29s 92ms/step - loss: 0.6308 - accuracy: 0.672 - 29s 91ms/step - loss: 0.6303 - accuracy: 0.672 - 29s 91ms/step - loss: 0.6299 - accuracy: 0.672 - 29s 91ms/step - loss: 0.6288 - accuracy: 0.673 - 29s 91ms/step - loss: 0.6287 - accuracy: 0.673 - 29s 91ms/step - loss: 0.6285 - accuracy: 0.673 - 29s 90ms/step - loss: 0.6284 - accuracy: 0.674 - 29s 90ms/step - loss: 0.6279 - accuracy: 0.674 - 29s 90ms/step - loss: 0.6276 - accuracy: 0.674 - 29s 90ms/step - loss: 0.6272 - accuracy: 0.675 - 29s 90ms/step - loss: 0.6269 - accuracy: 0.675 - 29s 90ms/step - loss: 0.6264 - accuracy: 0.675 - 29s 89ms/step - loss: 0.6259 - accuracy: 0.676 - 29s 89ms/step - loss: 0.6252 - accuracy: 0.676 - 29s 89ms/step - loss: 0.6244 - accuracy: 0.676 - 29s 89ms/step - loss: 0.6241 - accuracy: 0.677 - 29s 89ms/step - loss: 0.6240 - accuracy: 0.677 - 30s 89ms/step - loss: 0.6235 - accuracy: 0.677 - 30s 89ms/step - loss: 0.6229 - accuracy: 0.678 - 30s 88ms/step - loss: 0.6227 - accuracy: 0.6783"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    502/Unknown - 30s 88ms/step - loss: 0.6220 - accuracy: 0.678 - 30s 88ms/step - loss: 0.6220 - accuracy: 0.678 - 30s 88ms/step - loss: 0.6217 - accuracy: 0.678 - 30s 88ms/step - loss: 0.6215 - accuracy: 0.678 - 30s 88ms/step - loss: 0.6212 - accuracy: 0.679 - 30s 87ms/step - loss: 0.6205 - accuracy: 0.679 - 30s 87ms/step - loss: 0.6200 - accuracy: 0.680 - 30s 87ms/step - loss: 0.6196 - accuracy: 0.680 - 30s 87ms/step - loss: 0.6194 - accuracy: 0.680 - 30s 87ms/step - loss: 0.6187 - accuracy: 0.681 - 30s 87ms/step - loss: 0.6186 - accuracy: 0.681 - 30s 87ms/step - loss: 0.6185 - accuracy: 0.681 - 30s 86ms/step - loss: 0.6178 - accuracy: 0.681 - 30s 86ms/step - loss: 0.6176 - accuracy: 0.681 - 30s 86ms/step - loss: 0.6170 - accuracy: 0.682 - 30s 86ms/step - loss: 0.6164 - accuracy: 0.682 - 30s 86ms/step - loss: 0.6160 - accuracy: 0.683 - 30s 86ms/step - loss: 0.6156 - accuracy: 0.683 - 30s 86ms/step - loss: 0.6153 - accuracy: 0.683 - 30s 85ms/step - loss: 0.6144 - accuracy: 0.683 - 30s 85ms/step - loss: 0.6140 - accuracy: 0.683 - 30s 85ms/step - loss: 0.6137 - accuracy: 0.684 - 30s 85ms/step - loss: 0.6136 - accuracy: 0.684 - 30s 85ms/step - loss: 0.6132 - accuracy: 0.684 - 30s 85ms/step - loss: 0.6127 - accuracy: 0.685 - 31s 85ms/step - loss: 0.6119 - accuracy: 0.685 - 31s 84ms/step - loss: 0.6117 - accuracy: 0.685 - 31s 84ms/step - loss: 0.6110 - accuracy: 0.686 - 31s 84ms/step - loss: 0.6103 - accuracy: 0.686 - 31s 84ms/step - loss: 0.6102 - accuracy: 0.686 - 31s 84ms/step - loss: 0.6101 - accuracy: 0.686 - 31s 84ms/step - loss: 0.6094 - accuracy: 0.687 - 31s 84ms/step - loss: 0.6089 - accuracy: 0.687 - 31s 83ms/step - loss: 0.6081 - accuracy: 0.688 - 31s 83ms/step - loss: 0.6078 - accuracy: 0.688 - 31s 83ms/step - loss: 0.6077 - accuracy: 0.688 - 31s 83ms/step - loss: 0.6071 - accuracy: 0.688 - 31s 83ms/step - loss: 0.6065 - accuracy: 0.689 - 31s 83ms/step - loss: 0.6061 - accuracy: 0.689 - 31s 83ms/step - loss: 0.6055 - accuracy: 0.689 - 31s 83ms/step - loss: 0.6052 - accuracy: 0.689 - 31s 82ms/step - loss: 0.6052 - accuracy: 0.690 - 31s 82ms/step - loss: 0.6048 - accuracy: 0.690 - 31s 82ms/step - loss: 0.6045 - accuracy: 0.690 - 31s 82ms/step - loss: 0.6036 - accuracy: 0.690 - 31s 82ms/step - loss: 0.6032 - accuracy: 0.691 - 31s 82ms/step - loss: 0.6029 - accuracy: 0.691 - 31s 82ms/step - loss: 0.6026 - accuracy: 0.691 - 31s 82ms/step - loss: 0.6024 - accuracy: 0.691 - 31s 81ms/step - loss: 0.6021 - accuracy: 0.691 - 31s 81ms/step - loss: 0.6019 - accuracy: 0.692 - 31s 81ms/step - loss: 0.6012 - accuracy: 0.692 - 31s 81ms/step - loss: 0.6009 - accuracy: 0.692 - 31s 81ms/step - loss: 0.6005 - accuracy: 0.693 - 32s 81ms/step - loss: 0.6002 - accuracy: 0.693 - 32s 81ms/step - loss: 0.5996 - accuracy: 0.693 - 32s 81ms/step - loss: 0.5992 - accuracy: 0.694 - 32s 80ms/step - loss: 0.5989 - accuracy: 0.694 - 32s 80ms/step - loss: 0.5987 - accuracy: 0.694 - 32s 80ms/step - loss: 0.5984 - accuracy: 0.694 - 32s 80ms/step - loss: 0.5982 - accuracy: 0.694 - 32s 80ms/step - loss: 0.5978 - accuracy: 0.695 - 32s 80ms/step - loss: 0.5975 - accuracy: 0.695 - 32s 80ms/step - loss: 0.5971 - accuracy: 0.695 - 32s 80ms/step - loss: 0.5967 - accuracy: 0.695 - 32s 80ms/step - loss: 0.5966 - accuracy: 0.696 - 32s 79ms/step - loss: 0.5964 - accuracy: 0.696 - 32s 79ms/step - loss: 0.5964 - accuracy: 0.696 - 32s 79ms/step - loss: 0.5964 - accuracy: 0.696 - 32s 79ms/step - loss: 0.5960 - accuracy: 0.696 - 32s 79ms/step - loss: 0.5958 - accuracy: 0.696 - 32s 79ms/step - loss: 0.5953 - accuracy: 0.697 - 32s 79ms/step - loss: 0.5951 - accuracy: 0.697 - 32s 79ms/step - loss: 0.5948 - accuracy: 0.697 - 32s 79ms/step - loss: 0.5949 - accuracy: 0.697 - 32s 78ms/step - loss: 0.5947 - accuracy: 0.697 - 32s 78ms/step - loss: 0.5944 - accuracy: 0.697 - 32s 78ms/step - loss: 0.5940 - accuracy: 0.698 - 32s 78ms/step - loss: 0.5937 - accuracy: 0.698 - 32s 78ms/step - loss: 0.5936 - accuracy: 0.698 - 32s 78ms/step - loss: 0.5934 - accuracy: 0.699 - 32s 78ms/step - loss: 0.5929 - accuracy: 0.699 - 33s 78ms/step - loss: 0.5926 - accuracy: 0.699 - 33s 78ms/step - loss: 0.5922 - accuracy: 0.700 - 33s 78ms/step - loss: 0.5916 - accuracy: 0.700 - 33s 77ms/step - loss: 0.5912 - accuracy: 0.700 - 33s 77ms/step - loss: 0.5909 - accuracy: 0.700 - 33s 77ms/step - loss: 0.5905 - accuracy: 0.701 - 33s 77ms/step - loss: 0.5900 - accuracy: 0.701 - 33s 77ms/step - loss: 0.5895 - accuracy: 0.701 - 33s 77ms/step - loss: 0.5890 - accuracy: 0.702 - 33s 77ms/step - loss: 0.5886 - accuracy: 0.702 - 33s 77ms/step - loss: 0.5879 - accuracy: 0.702 - 33s 77ms/step - loss: 0.5874 - accuracy: 0.703 - 33s 77ms/step - loss: 0.5871 - accuracy: 0.703 - 33s 77ms/step - loss: 0.5866 - accuracy: 0.703 - 33s 76ms/step - loss: 0.5864 - accuracy: 0.703 - 33s 76ms/step - loss: 0.5861 - accuracy: 0.704 - 33s 76ms/step - loss: 0.5856 - accuracy: 0.704 - 33s 76ms/step - loss: 0.5853 - accuracy: 0.704 - 33s 76ms/step - loss: 0.5846 - accuracy: 0.704 - 33s 76ms/step - loss: 0.5846 - accuracy: 0.705 - 33s 76ms/step - loss: 0.5846 - accuracy: 0.705 - 33s 76ms/step - loss: 0.5844 - accuracy: 0.705 - 33s 76ms/step - loss: 0.5840 - accuracy: 0.705 - 33s 76ms/step - loss: 0.5838 - accuracy: 0.705 - 33s 76ms/step - loss: 0.5835 - accuracy: 0.705 - 33s 75ms/step - loss: 0.5832 - accuracy: 0.705 - 33s 75ms/step - loss: 0.5829 - accuracy: 0.705 - 34s 75ms/step - loss: 0.5824 - accuracy: 0.706 - 34s 75ms/step - loss: 0.5824 - accuracy: 0.706 - 34s 75ms/step - loss: 0.5820 - accuracy: 0.706 - 34s 75ms/step - loss: 0.5814 - accuracy: 0.706 - 34s 75ms/step - loss: 0.5809 - accuracy: 0.707 - 34s 75ms/step - loss: 0.5806 - accuracy: 0.707 - 34s 75ms/step - loss: 0.5803 - accuracy: 0.707 - 34s 75ms/step - loss: 0.5800 - accuracy: 0.707 - 34s 75ms/step - loss: 0.5796 - accuracy: 0.708 - 34s 75ms/step - loss: 0.5792 - accuracy: 0.708 - 34s 74ms/step - loss: 0.5791 - accuracy: 0.708 - 34s 74ms/step - loss: 0.5789 - accuracy: 0.708 - 34s 74ms/step - loss: 0.5784 - accuracy: 0.708 - 34s 74ms/step - loss: 0.5780 - accuracy: 0.709 - 34s 74ms/step - loss: 0.5782 - accuracy: 0.709 - 34s 74ms/step - loss: 0.5779 - accuracy: 0.709 - 34s 74ms/step - loss: 0.5775 - accuracy: 0.709 - 34s 74ms/step - loss: 0.5774 - accuracy: 0.709 - 34s 74ms/step - loss: 0.5769 - accuracy: 0.709 - 34s 74ms/step - loss: 0.5763 - accuracy: 0.710 - 34s 74ms/step - loss: 0.5760 - accuracy: 0.710 - 34s 74ms/step - loss: 0.5756 - accuracy: 0.710 - 34s 73ms/step - loss: 0.5753 - accuracy: 0.711 - 34s 73ms/step - loss: 0.5751 - accuracy: 0.711 - 34s 73ms/step - loss: 0.5748 - accuracy: 0.711 - 34s 73ms/step - loss: 0.5746 - accuracy: 0.711 - 34s 73ms/step - loss: 0.5743 - accuracy: 0.711 - 34s 73ms/step - loss: 0.5740 - accuracy: 0.711 - 35s 73ms/step - loss: 0.5737 - accuracy: 0.712 - 35s 73ms/step - loss: 0.5732 - accuracy: 0.712 - 35s 73ms/step - loss: 0.5730 - accuracy: 0.712 - 35s 73ms/step - loss: 0.5728 - accuracy: 0.712 - 35s 73ms/step - loss: 0.5725 - accuracy: 0.712 - 35s 73ms/step - loss: 0.5720 - accuracy: 0.713 - 35s 73ms/step - loss: 0.5719 - accuracy: 0.713 - 35s 72ms/step - loss: 0.5718 - accuracy: 0.713 - 35s 72ms/step - loss: 0.5714 - accuracy: 0.713 - 35s 72ms/step - loss: 0.5713 - accuracy: 0.713 - 35s 72ms/step - loss: 0.5710 - accuracy: 0.713 - 35s 72ms/step - loss: 0.5708 - accuracy: 0.713 - 35s 72ms/step - loss: 0.5706 - accuracy: 0.713 - 35s 72ms/step - loss: 0.5704 - accuracy: 0.713 - 35s 72ms/step - loss: 0.5699 - accuracy: 0.714 - 35s 72ms/step - loss: 0.5698 - accuracy: 0.714 - 35s 72ms/step - loss: 0.5695 - accuracy: 0.714 - 35s 72ms/step - loss: 0.5690 - accuracy: 0.714 - 35s 72ms/step - loss: 0.5690 - accuracy: 0.714 - 35s 72ms/step - loss: 0.5690 - accuracy: 0.714 - 35s 72ms/step - loss: 0.5689 - accuracy: 0.714 - 35s 71ms/step - loss: 0.5687 - accuracy: 0.715 - 35s 71ms/step - loss: 0.5685 - accuracy: 0.715 - 35s 71ms/step - loss: 0.5683 - accuracy: 0.715 - 35s 71ms/step - loss: 0.5681 - accuracy: 0.715 - 35s 71ms/step - loss: 0.5676 - accuracy: 0.715 - 35s 71ms/step - loss: 0.5673 - accuracy: 0.716 - 35s 71ms/step - loss: 0.5670 - accuracy: 0.716 - 36s 71ms/step - loss: 0.5666 - accuracy: 0.716 - 36s 71ms/step - loss: 0.5662 - accuracy: 0.7165"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    669/Unknown - 36s 71ms/step - loss: 0.5660 - accuracy: 0.716 - 36s 71ms/step - loss: 0.5657 - accuracy: 0.716 - 36s 71ms/step - loss: 0.5654 - accuracy: 0.717 - 36s 71ms/step - loss: 0.5651 - accuracy: 0.717 - 36s 70ms/step - loss: 0.5646 - accuracy: 0.717 - 36s 70ms/step - loss: 0.5645 - accuracy: 0.717 - 36s 70ms/step - loss: 0.5641 - accuracy: 0.717 - 36s 70ms/step - loss: 0.5637 - accuracy: 0.718 - 36s 70ms/step - loss: 0.5634 - accuracy: 0.718 - 36s 70ms/step - loss: 0.5631 - accuracy: 0.718 - 36s 70ms/step - loss: 0.5629 - accuracy: 0.718 - 36s 70ms/step - loss: 0.5623 - accuracy: 0.719 - 36s 70ms/step - loss: 0.5620 - accuracy: 0.719 - 36s 70ms/step - loss: 0.5619 - accuracy: 0.719 - 36s 70ms/step - loss: 0.5618 - accuracy: 0.719 - 36s 70ms/step - loss: 0.5614 - accuracy: 0.719 - 36s 70ms/step - loss: 0.5611 - accuracy: 0.719 - 36s 70ms/step - loss: 0.5607 - accuracy: 0.719 - 36s 70ms/step - loss: 0.5605 - accuracy: 0.719 - 36s 69ms/step - loss: 0.5605 - accuracy: 0.720 - 36s 69ms/step - loss: 0.5601 - accuracy: 0.720 - 36s 69ms/step - loss: 0.5597 - accuracy: 0.720 - 36s 69ms/step - loss: 0.5593 - accuracy: 0.720 - 36s 69ms/step - loss: 0.5593 - accuracy: 0.720 - 36s 69ms/step - loss: 0.5593 - accuracy: 0.720 - 36s 69ms/step - loss: 0.5589 - accuracy: 0.720 - 36s 69ms/step - loss: 0.5585 - accuracy: 0.721 - 37s 69ms/step - loss: 0.5584 - accuracy: 0.721 - 37s 69ms/step - loss: 0.5581 - accuracy: 0.721 - 37s 69ms/step - loss: 0.5580 - accuracy: 0.721 - 37s 69ms/step - loss: 0.5575 - accuracy: 0.721 - 37s 69ms/step - loss: 0.5573 - accuracy: 0.721 - 37s 69ms/step - loss: 0.5570 - accuracy: 0.722 - 37s 69ms/step - loss: 0.5567 - accuracy: 0.722 - 37s 69ms/step - loss: 0.5565 - accuracy: 0.722 - 37s 68ms/step - loss: 0.5561 - accuracy: 0.722 - 37s 68ms/step - loss: 0.5561 - accuracy: 0.722 - 37s 68ms/step - loss: 0.5557 - accuracy: 0.722 - 37s 68ms/step - loss: 0.5553 - accuracy: 0.723 - 37s 68ms/step - loss: 0.5550 - accuracy: 0.723 - 37s 68ms/step - loss: 0.5548 - accuracy: 0.723 - 37s 68ms/step - loss: 0.5548 - accuracy: 0.723 - 37s 68ms/step - loss: 0.5545 - accuracy: 0.723 - 37s 68ms/step - loss: 0.5541 - accuracy: 0.723 - 37s 68ms/step - loss: 0.5537 - accuracy: 0.723 - 37s 68ms/step - loss: 0.5535 - accuracy: 0.724 - 37s 68ms/step - loss: 0.5529 - accuracy: 0.724 - 37s 68ms/step - loss: 0.5527 - accuracy: 0.724 - 37s 68ms/step - loss: 0.5523 - accuracy: 0.724 - 37s 68ms/step - loss: 0.5520 - accuracy: 0.724 - 37s 68ms/step - loss: 0.5517 - accuracy: 0.724 - 37s 67ms/step - loss: 0.5515 - accuracy: 0.725 - 37s 67ms/step - loss: 0.5511 - accuracy: 0.725 - 37s 67ms/step - loss: 0.5506 - accuracy: 0.725 - 37s 67ms/step - loss: 0.5504 - accuracy: 0.725 - 38s 67ms/step - loss: 0.5499 - accuracy: 0.726 - 38s 67ms/step - loss: 0.5496 - accuracy: 0.726 - 38s 67ms/step - loss: 0.5498 - accuracy: 0.726 - 38s 67ms/step - loss: 0.5497 - accuracy: 0.726 - 38s 67ms/step - loss: 0.5492 - accuracy: 0.726 - 38s 67ms/step - loss: 0.5490 - accuracy: 0.726 - 38s 67ms/step - loss: 0.5488 - accuracy: 0.727 - 38s 67ms/step - loss: 0.5486 - accuracy: 0.727 - 38s 67ms/step - loss: 0.5483 - accuracy: 0.727 - 38s 67ms/step - loss: 0.5483 - accuracy: 0.727 - 38s 67ms/step - loss: 0.5480 - accuracy: 0.727 - 38s 67ms/step - loss: 0.5476 - accuracy: 0.727 - 38s 67ms/step - loss: 0.5472 - accuracy: 0.728 - 38s 67ms/step - loss: 0.5469 - accuracy: 0.728 - 38s 66ms/step - loss: 0.5465 - accuracy: 0.728 - 38s 66ms/step - loss: 0.5464 - accuracy: 0.728 - 38s 66ms/step - loss: 0.5461 - accuracy: 0.728 - 38s 66ms/step - loss: 0.5459 - accuracy: 0.728 - 38s 66ms/step - loss: 0.5455 - accuracy: 0.729 - 38s 66ms/step - loss: 0.5455 - accuracy: 0.729 - 38s 66ms/step - loss: 0.5453 - accuracy: 0.729 - 38s 66ms/step - loss: 0.5452 - accuracy: 0.729 - 38s 66ms/step - loss: 0.5450 - accuracy: 0.729 - 38s 66ms/step - loss: 0.5447 - accuracy: 0.729 - 38s 66ms/step - loss: 0.5445 - accuracy: 0.729 - 38s 66ms/step - loss: 0.5443 - accuracy: 0.729 - 38s 66ms/step - loss: 0.5440 - accuracy: 0.730 - 38s 66ms/step - loss: 0.5437 - accuracy: 0.730 - 39s 66ms/step - loss: 0.5437 - accuracy: 0.730 - 39s 66ms/step - loss: 0.5433 - accuracy: 0.730 - 39s 66ms/step - loss: 0.5430 - accuracy: 0.730 - 39s 66ms/step - loss: 0.5427 - accuracy: 0.731 - 39s 66ms/step - loss: 0.5423 - accuracy: 0.731 - 39s 65ms/step - loss: 0.5420 - accuracy: 0.731 - 39s 65ms/step - loss: 0.5419 - accuracy: 0.731 - 39s 65ms/step - loss: 0.5418 - accuracy: 0.731 - 39s 65ms/step - loss: 0.5415 - accuracy: 0.731 - 39s 65ms/step - loss: 0.5411 - accuracy: 0.731 - 39s 65ms/step - loss: 0.5408 - accuracy: 0.732 - 39s 65ms/step - loss: 0.5405 - accuracy: 0.732 - 39s 65ms/step - loss: 0.5407 - accuracy: 0.732 - 39s 65ms/step - loss: 0.5404 - accuracy: 0.732 - 39s 65ms/step - loss: 0.5404 - accuracy: 0.732 - 39s 65ms/step - loss: 0.5401 - accuracy: 0.732 - 39s 65ms/step - loss: 0.5397 - accuracy: 0.732 - 39s 65ms/step - loss: 0.5398 - accuracy: 0.732 - 39s 65ms/step - loss: 0.5395 - accuracy: 0.732 - 39s 65ms/step - loss: 0.5394 - accuracy: 0.733 - 39s 65ms/step - loss: 0.5391 - accuracy: 0.733 - 39s 65ms/step - loss: 0.5388 - accuracy: 0.733 - 39s 65ms/step - loss: 0.5385 - accuracy: 0.733 - 39s 65ms/step - loss: 0.5382 - accuracy: 0.733 - 39s 65ms/step - loss: 0.5381 - accuracy: 0.733 - 39s 65ms/step - loss: 0.5377 - accuracy: 0.733 - 39s 64ms/step - loss: 0.5374 - accuracy: 0.733 - 39s 64ms/step - loss: 0.5371 - accuracy: 0.734 - 40s 64ms/step - loss: 0.5371 - accuracy: 0.733 - 40s 64ms/step - loss: 0.5369 - accuracy: 0.734 - 40s 64ms/step - loss: 0.5368 - accuracy: 0.734 - 40s 64ms/step - loss: 0.5367 - accuracy: 0.734 - 40s 64ms/step - loss: 0.5363 - accuracy: 0.734 - 40s 64ms/step - loss: 0.5360 - accuracy: 0.734 - 40s 64ms/step - loss: 0.5359 - accuracy: 0.734 - 40s 64ms/step - loss: 0.5356 - accuracy: 0.734 - 40s 64ms/step - loss: 0.5354 - accuracy: 0.735 - 40s 64ms/step - loss: 0.5353 - accuracy: 0.735 - 40s 64ms/step - loss: 0.5351 - accuracy: 0.735 - 40s 64ms/step - loss: 0.5350 - accuracy: 0.735 - 40s 64ms/step - loss: 0.5348 - accuracy: 0.735 - 40s 64ms/step - loss: 0.5343 - accuracy: 0.735 - 40s 64ms/step - loss: 0.5341 - accuracy: 0.735 - 40s 64ms/step - loss: 0.5338 - accuracy: 0.735 - 40s 64ms/step - loss: 0.5336 - accuracy: 0.736 - 40s 64ms/step - loss: 0.5334 - accuracy: 0.736 - 40s 64ms/step - loss: 0.5331 - accuracy: 0.736 - 40s 64ms/step - loss: 0.5329 - accuracy: 0.736 - 40s 63ms/step - loss: 0.5326 - accuracy: 0.736 - 40s 63ms/step - loss: 0.5325 - accuracy: 0.736 - 40s 63ms/step - loss: 0.5321 - accuracy: 0.736 - 40s 63ms/step - loss: 0.5318 - accuracy: 0.737 - 40s 63ms/step - loss: 0.5315 - accuracy: 0.737 - 40s 63ms/step - loss: 0.5312 - accuracy: 0.737 - 40s 63ms/step - loss: 0.5309 - accuracy: 0.737 - 41s 63ms/step - loss: 0.5308 - accuracy: 0.737 - 41s 63ms/step - loss: 0.5307 - accuracy: 0.737 - 41s 63ms/step - loss: 0.5305 - accuracy: 0.737 - 41s 63ms/step - loss: 0.5304 - accuracy: 0.738 - 41s 63ms/step - loss: 0.5303 - accuracy: 0.738 - 41s 63ms/step - loss: 0.5300 - accuracy: 0.738 - 41s 63ms/step - loss: 0.5297 - accuracy: 0.738 - 41s 63ms/step - loss: 0.5296 - accuracy: 0.738 - 41s 63ms/step - loss: 0.5293 - accuracy: 0.738 - 41s 63ms/step - loss: 0.5292 - accuracy: 0.738 - 41s 63ms/step - loss: 0.5289 - accuracy: 0.738 - 41s 63ms/step - loss: 0.5287 - accuracy: 0.739 - 41s 63ms/step - loss: 0.5284 - accuracy: 0.739 - 41s 63ms/step - loss: 0.5281 - accuracy: 0.739 - 41s 63ms/step - loss: 0.5279 - accuracy: 0.739 - 41s 63ms/step - loss: 0.5276 - accuracy: 0.739 - 41s 63ms/step - loss: 0.5276 - accuracy: 0.739 - 41s 63ms/step - loss: 0.5273 - accuracy: 0.740 - 41s 62ms/step - loss: 0.5271 - accuracy: 0.740 - 41s 62ms/step - loss: 0.5271 - accuracy: 0.740 - 41s 62ms/step - loss: 0.5268 - accuracy: 0.740 - 41s 62ms/step - loss: 0.5267 - accuracy: 0.740 - 41s 62ms/step - loss: 0.5266 - accuracy: 0.740 - 41s 62ms/step - loss: 0.5266 - accuracy: 0.740 - 41s 62ms/step - loss: 0.5264 - accuracy: 0.740 - 41s 62ms/step - loss: 0.5263 - accuracy: 0.740 - 41s 62ms/step - loss: 0.5260 - accuracy: 0.740 - 42s 62ms/step - loss: 0.5258 - accuracy: 0.740 - 42s 62ms/step - loss: 0.5256 - accuracy: 0.7410"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697/697 [==============================]0.5255 - accuracy: 0.741 - 42s 62ms/step - loss: 0.5252 - accuracy: 0.741 - 42s 62ms/step - loss: 0.5250 - accuracy: 0.741 - 42s 62ms/step - loss: 0.5247 - accuracy: 0.741 - 42s 62ms/step - loss: 0.5245 - accuracy: 0.741 - 42s 62ms/step - loss: 0.5243 - accuracy: 0.741 - 42s 62ms/step - loss: 0.5240 - accuracy: 0.742 - 42s 62ms/step - loss: 0.5238 - accuracy: 0.742 - 42s 62ms/step - loss: 0.5237 - accuracy: 0.742 - 42s 62ms/step - loss: 0.5236 - accuracy: 0.742 - 42s 62ms/step - loss: 0.5234 - accuracy: 0.742 - 42s 62ms/step - loss: 0.5230 - accuracy: 0.742 - 42s 62ms/step - loss: 0.5228 - accuracy: 0.742 - 42s 62ms/step - loss: 0.5226 - accuracy: 0.742 - 42s 62ms/step - loss: 0.5223 - accuracy: 0.742 - 42s 62ms/step - loss: 0.5223 - accuracy: 0.742 - 42s 61ms/step - loss: 0.5222 - accuracy: 0.743 - 42s 61ms/step - loss: 0.5219 - accuracy: 0.743 - 42s 61ms/step - loss: 0.5218 - accuracy: 0.743 - 42s 61ms/step - loss: 0.5218 - accuracy: 0.743 - 42s 61ms/step - loss: 0.5216 - accuracy: 0.743 - 42s 61ms/step - loss: 0.5213 - accuracy: 0.743 - 42s 61ms/step - loss: 0.5210 - accuracy: 0.743 - 42s 61ms/step - loss: 0.5208 - accuracy: 0.743 - 42s 61ms/step - loss: 0.5206 - accuracy: 0.744 - 42s 61ms/step - loss: 0.5203 - accuracy: 0.744 - 43s 61ms/step - loss: 0.5203 - accuracy: 0.744 - 43s 61ms/step - loss: 0.5202 - accuracy: 0.744 - 47s 68ms/step - loss: 0.5202 - accuracy: 0.7444 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "697/697 [==============================] ETA: 17:18 - loss: 0.3870 - accuracy: 0.781 - ETA: 5:38 - loss: 0.3052 - accuracy: 0.843 - ETA: 3:18 - loss: 0.3174 - accuracy: 0.85 - ETA: 2:18 - loss: 0.2930 - accuracy: 0.86 - ETA: 1:45 - loss: 0.2967 - accuracy: 0.86 - ETA: 1:24 - loss: 0.2929 - accuracy: 0.87 - ETA: 1:09 - loss: 0.2982 - accuracy: 0.86 - ETA: 58s - loss: 0.2858 - accuracy: 0.8708 - ETA: 50s - loss: 0.2830 - accuracy: 0.872 - ETA: 43s - loss: 0.2912 - accuracy: 0.868 - ETA: 38s - loss: 0.2887 - accuracy: 0.868 - ETA: 33s - loss: 0.2799 - accuracy: 0.872 - ETA: 30s - loss: 0.2783 - accuracy: 0.875 - ETA: 27s - loss: 0.2805 - accuracy: 0.875 - ETA: 24s - loss: 0.2781 - accuracy: 0.878 - ETA: 21s - loss: 0.2789 - accuracy: 0.879 - ETA: 19s - loss: 0.2783 - accuracy: 0.880 - ETA: 17s - loss: 0.2792 - accuracy: 0.879 - ETA: 16s - loss: 0.2829 - accuracy: 0.878 - ETA: 14s - loss: 0.2844 - accuracy: 0.877 - ETA: 13s - loss: 0.2876 - accuracy: 0.875 - ETA: 12s - loss: 0.2864 - accuracy: 0.875 - ETA: 11s - loss: 0.2842 - accuracy: 0.875 - ETA: 9s - loss: 0.2875 - accuracy: 0.873 - ETA: 8s - loss: 0.2890 - accuracy: 0.87 - ETA: 8s - loss: 0.2907 - accuracy: 0.87 - ETA: 7s - loss: 0.2887 - accuracy: 0.87 - ETA: 6s - loss: 0.2912 - accuracy: 0.87 - ETA: 5s - loss: 0.2894 - accuracy: 0.87 - ETA: 5s - loss: 0.2901 - accuracy: 0.87 - ETA: 4s - loss: 0.2899 - accuracy: 0.87 - ETA: 3s - loss: 0.2875 - accuracy: 0.87 - ETA: 3s - loss: 0.2899 - accuracy: 0.87 - ETA: 2s - loss: 0.2912 - accuracy: 0.87 - ETA: 2s - loss: 0.2897 - accuracy: 0.87 - ETA: 1s - loss: 0.2899 - accuracy: 0.87 - ETA: 1s - loss: 0.2901 - accuracy: 0.87 - ETA: 0s - loss: 0.2916 - accuracy: 0.87 - ETA: 0s - loss: 0.2912 - accuracy: 0.87 - 37s 52ms/step - loss: 0.2972 - accuracy: 0.8678 - val_loss: 0.3586 - val_accuracy: 0.8298\n",
      "Epoch 3/3\n",
      "697/697 [==============================] ETA: 17:23 - loss: 0.1891 - accuracy: 0.953 - ETA: 5:40 - loss: 0.2354 - accuracy: 0.916 - ETA: 3:19 - loss: 0.2494 - accuracy: 0.90 - ETA: 2:19 - loss: 0.2246 - accuracy: 0.91 - ETA: 1:45 - loss: 0.2339 - accuracy: 0.91 - ETA: 1:24 - loss: 0.2305 - accuracy: 0.90 - ETA: 1:09 - loss: 0.2260 - accuracy: 0.90 - ETA: 58s - loss: 0.2263 - accuracy: 0.9073 - ETA: 50s - loss: 0.2232 - accuracy: 0.907 - ETA: 43s - loss: 0.2186 - accuracy: 0.911 - ETA: 38s - loss: 0.2102 - accuracy: 0.914 - ETA: 34s - loss: 0.2072 - accuracy: 0.915 - ETA: 30s - loss: 0.2048 - accuracy: 0.916 - ETA: 27s - loss: 0.2086 - accuracy: 0.914 - ETA: 24s - loss: 0.2049 - accuracy: 0.917 - ETA: 22s - loss: 0.2018 - accuracy: 0.918 - ETA: 20s - loss: 0.1995 - accuracy: 0.919 - ETA: 18s - loss: 0.1987 - accuracy: 0.918 - ETA: 16s - loss: 0.1943 - accuracy: 0.920 - ETA: 14s - loss: 0.1943 - accuracy: 0.919 - ETA: 13s - loss: 0.1923 - accuracy: 0.920 - ETA: 12s - loss: 0.1937 - accuracy: 0.920 - ETA: 11s - loss: 0.1919 - accuracy: 0.922 - ETA: 10s - loss: 0.1898 - accuracy: 0.923 - ETA: 9s - loss: 0.1904 - accuracy: 0.921 - ETA: 8s - loss: 0.1933 - accuracy: 0.91 - ETA: 7s - loss: 0.1936 - accuracy: 0.91 - ETA: 6s - loss: 0.1931 - accuracy: 0.91 - ETA: 5s - loss: 0.1934 - accuracy: 0.92 - ETA: 5s - loss: 0.1944 - accuracy: 0.92 - ETA: 4s - loss: 0.1928 - accuracy: 0.92 - ETA: 3s - loss: 0.1928 - accuracy: 0.92 - ETA: 3s - loss: 0.1944 - accuracy: 0.92 - ETA: 2s - loss: 0.1945 - accuracy: 0.92 - ETA: 2s - loss: 0.1928 - accuracy: 0.92 - ETA: 1s - loss: 0.1925 - accuracy: 0.92 - ETA: 1s - loss: 0.1918 - accuracy: 0.92 - ETA: 0s - loss: 0.1909 - accuracy: 0.92 - ETA: 0s - loss: 0.1916 - accuracy: 0.92 - 37s 53ms/step - loss: 0.2164 - accuracy: 0.9035 - val_loss: 0.3798 - val_accuracy: 0.8262\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2276c45f4a8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, epochs=3, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KTPCYf_Jh6TH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================].3646 - accuracy: 0.85 - 2s 885ms/step - loss: 0.4579 - accuracy: 0.804 - 2s 597ms/step - loss: 0.4287 - accuracy: 0.822 - 2s 453ms/step - loss: 0.4008 - accuracy: 0.835 - 2s 367ms/step - loss: 0.3890 - accuracy: 0.840 - 2s 309ms/step - loss: 0.3952 - accuracy: 0.833 - 2s 268ms/step - loss: 0.4032 - accuracy: 0.828 - 2s 237ms/step - loss: 0.3974 - accuracy: 0.828 - 2s 214ms/step - loss: 0.4182 - accuracy: 0.817 - 2s 195ms/step - loss: 0.4143 - accuracy: 0.817 - 2s 179ms/step - loss: 0.4089 - accuracy: 0.821 - 2s 166ms/step - loss: 0.4261 - accuracy: 0.815 - 2s 155ms/step - loss: 0.4053 - accuracy: 0.824 - 2s 145ms/step - loss: 0.3983 - accuracy: 0.825 - 2s 137ms/step - loss: 0.3904 - accuracy: 0.829 - 2s 130ms/step - loss: 0.3902 - accuracy: 0.827 - 2s 123ms/step - loss: 0.3875 - accuracy: 0.827 - 2s 118ms/step - loss: 0.3830 - accuracy: 0.828 - 2s 113ms/step - loss: 0.3808 - accuracy: 0.827 - 2s 108ms/step - loss: 0.3770 - accuracy: 0.827 - 2s 104ms/step - loss: 0.3749 - accuracy: 0.830 - 2s 101ms/step - loss: 0.3716 - accuracy: 0.831 - 2s 97ms/step - loss: 0.3649 - accuracy: 0.833 - 2s 94ms/step - loss: 0.3639 - accuracy: 0.83 - 2s 91ms/step - loss: 0.3682 - accuracy: 0.82 - 2s 89ms/step - loss: 0.3669 - accuracy: 0.82 - 2s 86ms/step - loss: 0.3682 - accuracy: 0.82 - 2s 84ms/step - loss: 0.3741 - accuracy: 0.82 - 2s 82ms/step - loss: 0.3756 - accuracy: 0.82 - 2s 80ms/step - loss: 0.3741 - accuracy: 0.82 - 2s 78ms/step - loss: 0.3759 - accuracy: 0.82 - 2s 76ms/step - loss: 0.3702 - accuracy: 0.82 - 2s 74ms/step - loss: 0.3732 - accuracy: 0.82 - 2s 73ms/step - loss: 0.3697 - accuracy: 0.82 - 3s 72ms/step - loss: 0.3706 - accuracy: 0.82 - 3s 70ms/step - loss: 0.3728 - accuracy: 0.82 - 3s 69ms/step - loss: 0.3702 - accuracy: 0.82 - 3s 68ms/step - loss: 0.3696 - accuracy: 0.82 - 3s 67ms/step - loss: 0.3696 - accuracy: 0.82 - 3s 66ms/step - loss: 0.3672 - accuracy: 0.82 - 3s 65ms/step - loss: 0.3639 - accuracy: 0.82 - 3s 64ms/step - loss: 0.3637 - accuracy: 0.82 - 3s 63ms/step - loss: 0.3630 - accuracy: 0.82 - 3s 62ms/step - loss: 0.3629 - accuracy: 0.82 - 3s 61ms/step - loss: 0.3655 - accuracy: 0.82 - 3s 60ms/step - loss: 0.3691 - accuracy: 0.82 - 3s 59ms/step - loss: 0.3682 - accuracy: 0.82 - 3s 59ms/step - loss: 0.3685 - accuracy: 0.82 - 3s 58ms/step - loss: 0.3678 - accuracy: 0.82 - 3s 57ms/step - loss: 0.3713 - accuracy: 0.82 - 3s 56ms/step - loss: 0.3705 - accuracy: 0.82 - 3s 56ms/step - loss: 0.3724 - accuracy: 0.82 - 3s 55ms/step - loss: 0.3749 - accuracy: 0.82 - 3s 55ms/step - loss: 0.3774 - accuracy: 0.82 - 3s 54ms/step - loss: 0.3757 - accuracy: 0.82 - 3s 53ms/step - loss: 0.3754 - accuracy: 0.82 - 3s 53ms/step - loss: 0.3758 - accuracy: 0.82 - 3s 52ms/step - loss: 0.3769 - accuracy: 0.82 - 3s 52ms/step - loss: 0.3768 - accuracy: 0.82 - 3s 51ms/step - loss: 0.3736 - accuracy: 0.82 - 3s 51ms/step - loss: 0.3785 - accuracy: 0.82 - 3s 51ms/step - loss: 0.3794 - accuracy: 0.82 - 3s 50ms/step - loss: 0.3817 - accuracy: 0.82 - 3s 50ms/step - loss: 0.3832 - accuracy: 0.82 - 3s 49ms/step - loss: 0.3835 - accuracy: 0.82 - 3s 49ms/step - loss: 0.3831 - accuracy: 0.82 - 3s 48ms/step - loss: 0.3818 - accuracy: 0.82 - 3s 48ms/step - loss: 0.3804 - accuracy: 0.82 - 3s 48ms/step - loss: 0.3786 - accuracy: 0.82 - 3s 47ms/step - loss: 0.3784 - accuracy: 0.82 - 3s 47ms/step - loss: 0.3795 - accuracy: 0.82 - 3s 47ms/step - loss: 0.3807 - accuracy: 0.82 - 3s 46ms/step - loss: 0.3808 - accuracy: 0.82 - 3s 46ms/step - loss: 0.3801 - accuracy: 0.82 - 3s 45ms/step - loss: 0.3790 - accuracy: 0.82 - 3s 45ms/step - loss: 0.3799 - accuracy: 0.82 - 3s 45ms/step - loss: 0.3804 - accuracy: 0.82 - 3s 45ms/step - loss: 0.3792 - accuracy: 0.82 - 3s 44ms/step - loss: 0.3798 - accuracy: 0.82 - 4s 45ms/step - loss: 0.3798 - accuracy: 0.8262\n",
      "\n",
      "Eval loss: 0.3797863131459755, Eval accuracy: 0.826200008392334\n"
     ]
    }
   ],
   "source": [
    "eval_loss, eval_acc = model.evaluate(test_data)\n",
    "print('\\nEval loss: {}, Eval accuracy: {}'.format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
