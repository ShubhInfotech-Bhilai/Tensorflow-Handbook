{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sUtoed20cRJJ"
   },
   "source": [
    "# 用 tf.data 加载 CSV 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-3Xbt0FfGfs"
   },
   "source": [
    "这篇教程通过一个示例展示了怎样将 CSV 格式的数据加载进 `tf.data.Dataset`。\n",
    "\n",
    "这篇教程使用的是泰坦尼克号乘客的数据。模型会根据乘客的年龄、性别、票务舱和是否独自旅行等特征来预测乘客生还的可能性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgZ9gjmPfSnK"
   },
   "source": [
    "## 设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4dwMQVQMQWD"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # Colab only\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "baYFZMW_bJHh"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ncf5t6tgL5ZI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tf-datasets/titanic/train.csv\n",
      "32768/30874 [===============================]- ETA:  - ETA:  - 0s 2us/step\n",
      "Downloading data from https://storage.googleapis.com/tf-datasets/titanic/eval.csv\n",
      "16384/13049 [=====================================]  - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
    "\n",
    "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
    "test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ONE94qulk6S"
   },
   "outputs": [],
   "source": [
    "# 让 numpy 数据更易读。\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wuqj601Qw0Ml"
   },
   "source": [
    "## 加载数据\n",
    "\n",
    "开始的时候，我们通过打印 CSV 文件的前几行来了解文件的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "54Dv7mCrf9Yw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "!head {train_file_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YOYKQKmMj3D6"
   },
   "source": [
    "正如你看到的那样，CSV 文件的每列都会有一个列名。dataset 的构造函数会自动识别这些列名。如果你使用的文件的第一行不包含列名，那么需要将列名通过字符串列表传给 `make_csv_dataset` 函数的 `column_names` 参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZS-bt1LvWn2x"
   },
   "source": [
    " \n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "CSV_COLUMNS = ['survived', 'sex', 'age', 'n_siblings_spouses', 'parch', 'fare', 'class', 'deck', 'embark_town', 'alone']\n",
    "\n",
    "dataset = tf.data.experimental.make_csv_dataset(\n",
    "     ...,\n",
    "     column_names=CSV_COLUMNS,\n",
    "     ...)\n",
    "  \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gZfhoX7bR9u4"
   },
   "source": [
    "这个示例使用了所有的列。如果你需要忽略数据集中的某些列，创建一个包含你需要使用的列的列表，然后传给构造器的（可选）参数 `select_columns`。\n",
    "\n",
    "```python\n",
    "\n",
    "dataset = tf.data.experimental.make_csv_dataset(\n",
    "  ...,\n",
    "  select_columns = columns_to_use, \n",
    "  ...)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67mfwr4v-mN_"
   },
   "source": [
    "对于包含模型需要预测的值的列是你需要显式指定的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iXROZm5f3V4E"
   },
   "outputs": [],
   "source": [
    "LABEL_COLUMN = 'survived'\n",
    "LABELS = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t4N-plO4tDXd"
   },
   "source": [
    "现在从文件中读取 CSV 数据并且创建 dataset。\n",
    "\n",
    "(完整的文档，参考 `tf.data.experimental.make_csv_dataset`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Co7UJ7gpNADC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\wangxingda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\data\\experimental\\ops\\readers.py:521: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n"
     ]
    }
   ],
   "source": [
    "def get_dataset(file_path):\n",
    "  dataset = tf.data.experimental.make_csv_dataset(\n",
    "      file_path,\n",
    "      batch_size=12, # 为了示例更容易展示，手动设置较小的值\n",
    "      label_name=LABEL_COLUMN,\n",
    "      na_value=\"?\",\n",
    "      num_epochs=1,\n",
    "      ignore_errors=True)\n",
    "  return dataset\n",
    "\n",
    "raw_train_data = get_dataset(train_file_path)\n",
    "raw_test_data = get_dataset(test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHUQFKoQI6G7"
   },
   "source": [
    "dataset 中的每个条目都是一个批次，用一个元组（*多个样本*，*多个标签*）表示。样本中的数据组织形式是以列为主的张量（而不是以行为主的张量），每条数据中包含的元素个数就是批次大小（这个示例中是 12）。\n",
    "\n",
    "阅读下面的示例有助于你的理解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qWtFYtwXIeuj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLES: \n",
      " OrderedDict([('sex', <tf.Tensor: id=176, shape=(12,), dtype=string, numpy=\n",
      "array([b'male', b'female', b'male', b'male', b'female', b'male', b'male',\n",
      "       b'male', b'male', b'male', b'female', b'female'], dtype=object)>), ('age', <tf.Tensor: id=168, shape=(12,), dtype=float32, numpy=\n",
      "array([40.5, 28. , 28. , 11. , 28. , 40. , 28. , 18. , 22. , 28. , 22. ,\n",
      "       28. ], dtype=float32)>), ('n_siblings_spouses', <tf.Tensor: id=174, shape=(12,), dtype=int32, numpy=array([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0])>), ('parch', <tf.Tensor: id=175, shape=(12,), dtype=int32, numpy=array([0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])>), ('fare', <tf.Tensor: id=173, shape=(12,), dtype=float32, numpy=\n",
      "array([ 7.75 ,  7.75 ,  0.   , 18.788, 24.15 ,  0.   ,  7.55 ,  6.496,\n",
      "        7.25 ,  8.05 , 55.   ,  7.787], dtype=float32)>), ('class', <tf.Tensor: id=170, shape=(12,), dtype=string, numpy=\n",
      "array([b'Third', b'Third', b'Second', b'Third', b'Third', b'First',\n",
      "       b'Third', b'Third', b'Third', b'Third', b'First', b'Third'],\n",
      "      dtype=object)>), ('deck', <tf.Tensor: id=171, shape=(12,), dtype=string, numpy=\n",
      "array([b'unknown', b'unknown', b'unknown', b'unknown', b'unknown', b'B',\n",
      "       b'unknown', b'unknown', b'unknown', b'unknown', b'E', b'unknown'],\n",
      "      dtype=object)>), ('embark_town', <tf.Tensor: id=172, shape=(12,), dtype=string, numpy=\n",
      "array([b'Queenstown', b'Queenstown', b'Southampton', b'Cherbourg',\n",
      "       b'Queenstown', b'Southampton', b'Southampton', b'Southampton',\n",
      "       b'Southampton', b'Southampton', b'Southampton', b'Queenstown'],\n",
      "      dtype=object)>), ('alone', <tf.Tensor: id=169, shape=(12,), dtype=string, numpy=\n",
      "array([b'y', b'n', b'y', b'y', b'n', b'y', b'y', b'n', b'y', b'y', b'n',\n",
      "       b'y'], dtype=object)>)]) \n",
      "\n",
      "LABELS: \n",
      " tf.Tensor([0 0 0 0 1 0 0 0 0 0 1 1], shape=(12,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "examples, labels = next(iter(raw_train_data)) # 第一个批次\n",
    "print(\"EXAMPLES: \\n\", examples, \"\\n\")\n",
    "print(\"LABELS: \\n\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9cryz31lxs3e"
   },
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tSyrkSQwYHKi"
   },
   "source": [
    "### 分类数据\n",
    "\n",
    "CSV 数据中的有些列是分类的列。也就是说，这些列只能在有限的集合中取值。\n",
    "\n",
    "使用 `tf.feature_column` API 创建一个 `tf.feature_column.indicator_column` 集合，每个 `tf.feature_column.indicator_column` 对应一个分类的列。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mWDniduKMw-C"
   },
   "outputs": [],
   "source": [
    "CATEGORIES = {\n",
    "    'sex': ['male', 'female'],\n",
    "    'class' : ['First', 'Second', 'Third'],\n",
    "    'deck' : ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],\n",
    "    'embark_town' : ['Cherbourg', 'Southhampton', 'Queenstown'],\n",
    "    'alone' : ['y', 'n']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kkxLdrsLwHPT"
   },
   "outputs": [],
   "source": [
    "categorical_columns = []\n",
    "for feature, vocab in CATEGORIES.items():\n",
    "  cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key=feature, vocabulary_list=vocab)\n",
    "  categorical_columns.append(tf.feature_column.indicator_column(cat_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H18CxpHY_Nma"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='sex', vocabulary_list=('male', 'female'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='class', vocabulary_list=('First', 'Second', 'Third'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='deck', vocabulary_list=('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='embark_town', vocabulary_list=('Cherbourg', 'Southhampton', 'Queenstown'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='alone', vocabulary_list=('y', 'n'), dtype=tf.string, default_value=-1, num_oov_buckets=0))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 你刚才创建的内容\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R7-1QG99_1sN"
   },
   "source": [
    "这将是后续构建模型时处理输入数据的一部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9AsbaFmCeJtF"
   },
   "source": [
    "### 连续数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o2maE8d2ijsq"
   },
   "source": [
    "连续数据需要标准化。\n",
    "\n",
    "写一个函数标准化这些值，然后将这些值改造成 2 维的张量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "REKqO_xHPNx0"
   },
   "outputs": [],
   "source": [
    "def process_continuous_data(mean, data):\n",
    "  # 标准化数据\n",
    "  data = tf.cast(data, tf.float32) * 1/(2*mean)\n",
    "  return tf.reshape(data, [-1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPsoMUgRCpUM"
   },
   "source": [
    "现在创建一个数值列的集合。`tf.feature_columns.numeric_column` API 会使用 `normalizer_fn` 参数。在传参的时候使用  [`functools.partial`](https://docs.python.org/3/library/functools.html#functools.partial)，`functools.partial` 由使用每个列的均值进行标准化的函数构成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WKT1ASWpwH46"
   },
   "outputs": [],
   "source": [
    "MEANS = {\n",
    "    'age' : 29.631308,\n",
    "    'n_siblings_spouses' : 0.545455,\n",
    "    'parch' : 0.379585,\n",
    "    'fare' : 34.385399\n",
    "}\n",
    "\n",
    "numerical_columns = []\n",
    "\n",
    "for feature in MEANS.keys():\n",
    "  num_col = tf.feature_column.numeric_column(feature, normalizer_fn=functools.partial(process_continuous_data, MEANS[feature]))\n",
    "  numerical_columns.append(num_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bw0I35xRS57V"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function process_continuous_data at 0x0000017C89B75730>, 29.631308)),\n",
       " NumericColumn(key='n_siblings_spouses', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function process_continuous_data at 0x0000017C89B75730>, 0.545455)),\n",
       " NumericColumn(key='parch', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function process_continuous_data at 0x0000017C89B75730>, 0.379585)),\n",
       " NumericColumn(key='fare', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function process_continuous_data at 0x0000017C89B75730>, 34.385399))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 你刚才创建的内容。\n",
    "numerical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M37oD2VcCO4R"
   },
   "source": [
    "这里使用标准化的方法需要提前知道每列的均值。如果需要计算连续的数据流的标准化的值可以使用 [TensorFlow Transform](https://www.tensorflow.org/tfx/transform/get_started)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kPWkC4_1l3IG"
   },
   "source": [
    "### 创建预处理层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3QAjo1qD4p9"
   },
   "source": [
    "将这两个特征列的集合相加，并且传给 `tf.keras.layers.DenseFeatures` 从而创建一个进行预处理的输入层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3-OYK7GnaH0r"
   },
   "outputs": [],
   "source": [
    "preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns+numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlF_omQqtnOP"
   },
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lQoFh16LxtT_"
   },
   "source": [
    "从 `preprocessing_layer` 开始构建 `tf.keras.Sequential`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3mSGsHTFPvFo"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  preprocessing_layer,\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hPdtI2ie0lEZ"
   },
   "source": [
    "## 训练、评估和预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8gvw1RE9zXkD"
   },
   "source": [
    "现在可以实例化和训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sW-4XlLeEQ2B"
   },
   "outputs": [],
   "source": [
    "train_data = raw_train_data.shuffle(500)\n",
    "test_data = raw_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_nm28IzNDTO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\wangxingda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py:4276: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From c:\\users\\wangxingda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py:4331: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "Epoch 1/20\n",
      "53/53 [==============================]: 0.7810 - accuracy: 0.333 - 1s 467ms/step - loss: 0.7992 - accuracy: 0.266 - 1s 313ms/step - loss: 0.7767 - accuracy: 0.296 - 1s 236ms/step - loss: 0.7476 - accuracy: 0.384 - 1s 190ms/step - loss: 0.7384 - accuracy: 0.431 - 1s 159ms/step - loss: 0.7281 - accuracy: 0.476 - 1s 137ms/step - loss: 0.7228 - accuracy: 0.480 - 1s 121ms/step - loss: 0.7111 - accuracy: 0.528 - 1s 108ms/step - loss: 0.7051 - accuracy: 0.555 - 1s 98ms/step - loss: 0.6945 - accuracy: 0.576 - 1s 90ms/step - loss: 0.6827 - accuracy: 0.61 - 1s 83ms/step - loss: 0.6845 - accuracy: 0.60 - 1s 77ms/step - loss: 0.6810 - accuracy: 0.60 - 1s 72ms/step - loss: 0.6751 - accuracy: 0.62 - 1s 67ms/step - loss: 0.6732 - accuracy: 0.62 - 1s 63ms/step - loss: 0.6669 - accuracy: 0.63 - 1s 60ms/step - loss: 0.6623 - accuracy: 0.63 - 1s 57ms/step - loss: 0.6517 - accuracy: 0.65 - 1s 54ms/step - loss: 0.6568 - accuracy: 0.63 - 1s 52ms/step - loss: 0.6613 - accuracy: 0.63 - 1s 49ms/step - loss: 0.6559 - accuracy: 0.64 - 1s 47ms/step - loss: 0.6481 - accuracy: 0.65 - 1s 46ms/step - loss: 0.6481 - accuracy: 0.66 - 1s 44ms/step - loss: 0.6431 - accuracy: 0.66 - 1s 42ms/step - loss: 0.6370 - accuracy: 0.66 - 1s 41ms/step - loss: 0.6339 - accuracy: 0.67 - 1s 40ms/step - loss: 0.6335 - accuracy: 0.66 - 1s 38ms/step - loss: 0.6320 - accuracy: 0.66 - 1s 37ms/step - loss: 0.6296 - accuracy: 0.67 - 1s 36ms/step - loss: 0.6277 - accuracy: 0.67 - 1s 35ms/step - loss: 0.6279 - accuracy: 0.67 - 1s 34ms/step - loss: 0.6223 - accuracy: 0.68 - 1s 33ms/step - loss: 0.6224 - accuracy: 0.67 - 1s 32ms/step - loss: 0.6173 - accuracy: 0.68 - 1s 32ms/step - loss: 0.6170 - accuracy: 0.68 - 1s 31ms/step - loss: 0.6108 - accuracy: 0.69 - 1s 30ms/step - loss: 0.6069 - accuracy: 0.69 - 1s 30ms/step - loss: 0.6021 - accuracy: 0.70 - 1s 29ms/step - loss: 0.5988 - accuracy: 0.70 - 1s 28ms/step - loss: 0.5961 - accuracy: 0.71 - 1s 28ms/step - loss: 0.5948 - accuracy: 0.71 - 1s 27ms/step - loss: 0.5903 - accuracy: 0.71 - 1s 27ms/step - loss: 0.5883 - accuracy: 0.71 - 1s 26ms/step - loss: 0.5885 - accuracy: 0.71 - 1s 26ms/step - loss: 0.5854 - accuracy: 0.71 - 1s 25ms/step - loss: 0.5833 - accuracy: 0.72 - 1s 25ms/step - loss: 0.5813 - accuracy: 0.72 - 1s 24ms/step - loss: 0.5819 - accuracy: 0.72 - 1s 24ms/step - loss: 0.5758 - accuracy: 0.72 - 1s 24ms/step - loss: 0.5729 - accuracy: 0.73 - 1s 23ms/step - loss: 0.5696 - accuracy: 0.73 - 1s 23ms/step - loss: 0.5654 - accuracy: 0.73 - 1s 23ms/step - loss: 0.5618 - accuracy: 0.73 - 2s 29ms/step - loss: 0.5618 - accuracy: 0.7384\n",
      "Epoch 2/20\n",
      "53/53 [==============================] - ETA: 1s - loss: 0.6491 - accuracy: 0.66 - ETA: 0s - loss: 0.4413 - accuracy: 0.79 - 0s 2ms/step - loss: 0.4357 - accuracy: 0.8054\n",
      "Epoch 3/20\n",
      "53/53 [==============================] - ETA: 1s - loss: 0.6712 - accuracy: 0.66 - ETA: 0s - loss: 0.4169 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4226 - accuracy: 0.8150\n",
      "Epoch 4/20\n",
      "53/53 [==============================] - ETA: 1s - loss: 0.3706 - accuracy: 0.83 - ETA: 0s - loss: 0.3574 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4077 - accuracy: 0.8102\n",
      "Epoch 5/20\n",
      "53/53 [==============================] - ETA: 1s - loss: 0.2588 - accuracy: 0.91 - ETA: 0s - loss: 0.4017 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4147 - accuracy: 0.8309\n",
      "Epoch 6/20\n",
      "53/53 [==============================] - ETA: 1s - loss: 0.2563 - accuracy: 0.91 - ETA: 0s - loss: 0.3550 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3878 - accuracy: 0.8389\n",
      "Epoch 7/20\n",
      "53/53 [==============================] - ETA: 1s - loss: 0.3095 - accuracy: 0.91 - ETA: 0s - loss: 0.4117 - accuracy: 0.82 - 0s 2ms/step - loss: 0.3868 - accuracy: 0.8453\n",
      "Epoch 8/20\n",
      "53/53 [==============================] - ETA: 1s - loss: 0.3034 - accuracy: 0.83 - ETA: 0s - loss: 0.3794 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3849 - accuracy: 0.8357\n",
      "Epoch 9/20\n",
      "53/53 [==============================] - ETA: 1s - loss: 0.5622 - accuracy: 0.75 - ETA: 0s - loss: 0.3645 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3831 - accuracy: 0.8357\n",
      "Epoch 10/20\n",
      "53/53 [==============================] - ETA: 1s - loss: 0.3863 - accuracy: 0.83 - ETA: 0s - loss: 0.3740 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3746 - accuracy: 0.8357\n",
      "Epoch 11/20\n",
      "53/53 [==============================] - ETA: 1s - loss: 0.3239 - accuracy: 0.91 - ETA: 0s - loss: 0.3804 - accuracy: 0.82 - 0s 2ms/step - loss: 0.3676 - accuracy: 0.8341\n",
      "Epoch 12/20\n",
      "53/53 [==============================] - ETA: 1s - loss: 0.5896 - accuracy: 0.66 - ETA: 0s - loss: 0.3672 - accuracy: 0.83 - 0s 3ms/step - loss: 0.3608 - accuracy: 0.8501\n",
      "Epoch 13/20\n",
      "53/53 [==============================] - ETA: 1s - loss: 0.1389 - accuracy: 0.91 - ETA: 0s - loss: 0.3295 - accuracy: 0.85 - 0s 3ms/step - loss: 0.3659 - accuracy: 0.8437\n",
      "Epoch 14/20\n",
      "53/53 [==============================] - ETA: 1s - loss: 0.4302 - accuracy: 0.83 - ETA: 0s - loss: 0.3140 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3562 - accuracy: 0.8469\n",
      "Epoch 15/20\n",
      "53/53 [==============================] - ETA: 1s - loss: 0.2017 - accuracy: 0.91 - ETA: 0s - loss: 0.3442 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3458 - accuracy: 0.8469\n",
      "Epoch 16/20\n",
      "53/53 [==============================] - ETA: 1s - loss: 0.3165 - accuracy: 0.91 - ETA: 0s - loss: 0.3413 - accuracy: 0.85 - 0s 3ms/step - loss: 0.3430 - accuracy: 0.8485\n",
      "Epoch 17/20\n",
      "53/53 [==============================] - ETA: 1s - loss: 0.3910 - accuracy: 0.83 - ETA: 0s - loss: 0.3724 - accuracy: 0.85 - 0s 3ms/step - loss: 0.3686 - accuracy: 0.8596\n",
      "Epoch 18/20\n",
      "53/53 [==============================] - ETA: 1s - loss: 0.1893 - accuracy: 1.00 - ETA: 0s - loss: 0.3517 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3515 - accuracy: 0.8565\n",
      "Epoch 19/20\n",
      "53/53 [==============================] - ETA: 1s - loss: 0.3315 - accuracy: 0.75 - ETA: 0s - loss: 0.3247 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3493 - accuracy: 0.8565\n",
      "Epoch 20/20\n",
      "53/53 [==============================] - ETA: 1s - loss: 0.4840 - accuracy: 0.66 - ETA: 0s - loss: 0.3687 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3339 - accuracy: 0.8612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17c8ae532b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QyDMgBurzqQo"
   },
   "source": [
    "当模型训练完成的时候，你可以在测试集 `test_data` 上检查准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eB3R3ViVONOp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================]: 0.6116 - accuracy: 0.583 - 0s 105ms/step - loss: 0.6170 - accuracy: 0.666 - 0s 71ms/step - loss: 0.4996 - accuracy: 0.750 - 0s 55ms/step - loss: 0.5170 - accuracy: 0.75 - 0s 44ms/step - loss: 0.4966 - accuracy: 0.75 - 0s 38ms/step - loss: 0.4941 - accuracy: 0.75 - 0s 33ms/step - loss: 0.4504 - accuracy: 0.77 - 0s 29ms/step - loss: 0.4474 - accuracy: 0.78 - 0s 27ms/step - loss: 0.4333 - accuracy: 0.79 - 0s 24ms/step - loss: 0.4192 - accuracy: 0.80 - 0s 22ms/step - loss: 0.3951 - accuracy: 0.82 - 0s 21ms/step - loss: 0.3905 - accuracy: 0.83 - 0s 20ms/step - loss: 0.4224 - accuracy: 0.82 - 0s 18ms/step - loss: 0.4198 - accuracy: 0.82 - 0s 17ms/step - loss: 0.4204 - accuracy: 0.82 - 0s 17ms/step - loss: 0.4077 - accuracy: 0.83 - 0s 16ms/step - loss: 0.4391 - accuracy: 0.82 - 0s 15ms/step - loss: 0.4460 - accuracy: 0.81 - 0s 15ms/step - loss: 0.4355 - accuracy: 0.82 - 0s 14ms/step - loss: 0.4378 - accuracy: 0.81 - 0s 14ms/step - loss: 0.4293 - accuracy: 0.82 - 0s 13ms/step - loss: 0.4369 - accuracy: 0.81 - 0s 14ms/step - loss: 0.4369 - accuracy: 0.8182\n",
      "\n",
      "\n",
      "Test Loss 0.4369204965504733, Test Accuracy 0.8181818127632141\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "\n",
    "print('\\n\\nTest Loss {}, Test Accuracy {}'.format(test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sTrn_pD90gdJ"
   },
   "source": [
    "使用 `tf.keras.Model.predict` 推断一个批次或多个批次的标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qwcx74F3ojqe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted survival: 99.60%  | Actual outcome:  DIED\n",
      "Predicted survival: 43.89%  | Actual outcome:  DIED\n",
      "Predicted survival: 44.95%  | Actual outcome:  SURVIVED\n",
      "Predicted survival: 2.34%  | Actual outcome:  DIED\n",
      "Predicted survival: 5.64%  | Actual outcome:  DIED\n",
      "Predicted survival: 38.27%  | Actual outcome:  DIED\n",
      "Predicted survival: 99.85%  | Actual outcome:  SURVIVED\n",
      "Predicted survival: 7.53%  | Actual outcome:  DIED\n",
      "Predicted survival: 99.58%  | Actual outcome:  DIED\n",
      "Predicted survival: 42.32%  | Actual outcome:  DIED\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_data)\n",
    "\n",
    "# 显示部分结果\n",
    "for prediction, survived in zip(predictions[:10], list(test_data)[0][1][:10]):\n",
    "  print(\"Predicted survival: {:.2%}\".format(prediction[0]),\n",
    "        \" | Actual outcome: \",\n",
    "        (\"SURVIVED\" if bool(survived) else \"DIED\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "csv.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
